__M3M6: Methods of Mathematical Physics__

Dr. Sheehan Olver

s.olver@imperial.ac.uk

# Lecture 8: Matrix  functions via Cauchy's integral formula



### Application: calculating matrix exponentials

Let $A \in {\mathbb C}^{d \times d}$,   ${\mathbf u}_0 \in {\mathbb C}^d$ and consider the constant coefficient linear ODE

$$
    {\mathbf u}'(t) = A {\mathbf u}(t)\qquad\hbox{and}\qquad {\mathbf u}(0) = {\mathbf u}_0(0)
$$
    
The solution to this is given by the _matrix exponential_
$$
    {\mathbf u}(t) = \exp(A t) {\mathbf u}_0
$$
    where the matrix exponential is defined by it's Taylor series:
$$
    \exp(A) = \sum_{k=0}^\infty {A^k \over k!}
$$
This has stability problems, so a more convenient form is as follows:

**Theorem (Cauchy integral representation for matrix exponential)**
Let $A \in {\mathbb C}^{n \times n}$ be a diagonalizable matrix:
$$
    A = V \Lambda V^{-1} \qquad\hbox{for}\qquad\Lambda =
    \begin{pmatrix}\lambda_1 \cr &\ddots \cr && \lambda_d\end{pmatrix}
$$
Let $\gamma$ be a contour that surrounds the spectrum of $A$. Then we have
$$
    \exp(A) = {1 \over 2 \pi i} \oint_\gamma e^z (z I - A)^{-1} dz
$$
    
**Proof** Note by definition
$$
\begin{align*}
\exp(A) &= \sum_{k=0}^\infty {A^k \over k!} = V \sum_{k=0}^\infty {\Lambda^k \over k!}V^{-1} = V \begin{pmatrix} \E^{\lambda_1}\\ & \ddots \\ && \E^{\lambda_d} \end{pmatrix} V^{-1}  \\
&= {1 \over 2\pi \I} V \begin{pmatrix} \oint_\gamma {\E^\zeta \over \zeta - \lambda_1 } \D\zeta \\ & \ddots \\ && \oint_\gamma {\E^\zeta \over \zeta - \lambda_d } \D\zeta\end{pmatrix} V^{-1} 
\end{align*}
$$
It was important here that $\gamma$ surrounded all $\zeta_j$.

We now take out the integration from the matrix, the easiest way to see this is to apply the Trapezium rule approximation:
$$
\begin{align*}
V\begin{pmatrix} \oint_\gamma {\E^\zeta \over \zeta - \lambda_1 } \D\zeta \\ & \ddots \\ && \oint_\gamma {\E^\zeta \over \zeta - \lambda_d } \D\zeta\end{pmatrix} V^{-1} &= V\lim_{n \rightarrow \infty} \begin{pmatrix} \sum_{j=1}^n {w_j \E^\zeta_j \over \zeta_j - \lambda_1 }  \\ & \ddots \\ && \sum_{j=1}^n {w_j \E^\zeta_j \over \zeta_j - \lambda_d } \end{pmatrix} V^{-1} \\
&= \lim_{n \rightarrow \infty}  V\sum_{j=1}^n w_j \E^\zeta_j \begin{pmatrix}  {1 \over \zeta_j - \lambda_1 }  \\ & \ddots \\ &&  {1 \over \zeta_j - \lambda_d } \end{pmatrix} V^{-1} \\
&= \lim_{n \rightarrow \infty}  \sum_{j=1}^n w_j \E^\zeta_j V(I \zeta_j - \Lambda)^{-1}V^{-1} \\
&= \oint_\gamma \E^\zeta V(I \zeta - \Lambda)^{-1} V^{-1} \D \zeta 
= \oint_\gamma \E^\zeta (I \zeta - V\Lambda V^{-1})^{-1}  \D \zeta \\
& = \oint_\gamma \E^\zeta (I \zeta - A)^{-1}  \D \zeta \\
\end{align*}
$$


■

_Demonstration_ we use this formula alongside the complex trapezium rule to calculate matrix exponentials. 
Begin by creating a random symmetric matrix (which only has real eigenvalues):
```julia
using LinearAlgebra

A = randn(5,5)
A = A + A'
λ = eigvals(A)
```
We can now by hand create a circle that surrounds all the eigenvalues:
```julia
function circle_rule(n, r) 
    θ = periodic_rule(n)[1]
    r*exp.(im*θ), 2π*im*r/n*exp.(im*θ)
end
z,w = circle_rule(100,maximum(abs.(λ))+1)

plot(z)
scatter!(λ,zeros(5); label = "eigenvalues of A")
```
Here we wrap this up into a function `circle_exp` that calculates the matrix exponential:
```julia
function circle_exp(A, n, z₀, r)
    z,w = circle_rule(n,r)
    z .+= z₀

    ret = zero(A)
    for j=1:n
        ret += w[j]*exp(z[j])*inv(z[j]*I - A)
    end

    ret/(2π*im)
end
   
circle_exp(A, 100, 0, 8.0) -exp(A) |>norm
```
In this case, it is beneficial to use an ellipse:
```julia
function ellipse_exp(A, n, z₀, a, b)
    z,w = ellipse_rule(n,a,b)
    z .+= z₀

    ret = zero(A)
    for j=1:n
        ret += w[j]*exp(z[j])*inv(z[j]*I - A)
    end
    ret/(2π*im)
end


ellipse_exp(A, 50, 0, 8.0, 5.0) -exp(A) |>norm
```
For matrices with large negative eigenvalues (For example, discretisations of the Laplacian), complex quadrature can lead to much better accuracy than Taylor series:
```julia
function taylor_exp(A,n)
    ret = Matrix(I, size(A))
    for k=1:n
        ret += A^k/factorial(1.0k)
    end
    ret
end

B = A - 20I

taylor_exp(B, 200) -exp(B) |>norm
```
We can use an ellpise to surround the spectrum:
```julia
scatter(complex.(eigvals(B)))
plot!(ellipse_rule(50,8,5)[1] .- 20)

norm(ellipse_exp(B, 50, -20.0, 8.0, 5.0) - exp(B))
```
### Gershgorin circle theorem

If we only know $A$, how do we know how big to make the contour? Gershgorin's circle theorem gives the answer:

**Theorem (Gershgorin)** Let $A \in {\mathbb C}^{d \times d}$ and define 
$$
R_k = \sum_{j=1 \atop j \neq k}^d |a_{kj}| 
$$
Then 
$$
\rho(A) \subset \bigcup_{k=1}^d \bar B(a_{kk}, R_k)
$$
where $\bar B(z_0, r)$ is the closed disk of radius $r$ and $\rho(A)$ is the set of eigenvalues.

**Proof **


■

_Demonstration_ Here we apply this to a particular matrix:
```julia
A = [1 2 3; 1 5 2; -4 1 6]
```
The following calculates the row sums:
```julia
R = sum(abs.(A - Diagonal(diag(A))),dims=2)
```
Gershgorin's theorem tells us that the spectrum lies in the union of the circles surrounding the diagonals:
```julia
drawcircle!(z0, R) = plot!(θ-> real(z0) + R[1]*cos(θ), θ-> imag(z0) + R[1]*sin(θ), 0, 2π, fill=(0,:red), α = 0.2, legend=false)

λ = eigvals(A)
p = plot()
for k = 1:size(A,1)
    drawcircle!(A[k,k], R[k])
end
scatter!(complex.(λ); label="eigenvalues")
scatter!(complex.(diag(A)); label="diagonals")
p
```
We can therefore use this to choose a contour big enough to surround all the circles. Here's a fairly simplistic construction for our case where everything is real:
```julia
z₀ = (maximum(diag(A) .+ R) + minimum(diag(A) .- R)) /2 # average edges of circle
r = max(abs.(diag(A) .- R .- z₀)..., abs.(diag(A) .+ R .- z₀)...)

plot!(Circle(z₀, r))
```

Here we consider two cases
```julia
N = 1000
h = 1/N
Δ = SymTridiagonal(fill(-2,N), fill(1,N-1))/h^2
eigvals(Δ)


u0 = x -> x * (1-x) * exp(x)

t = 0.1
xx = range(0,1; length=N+2)
A = Matrix(Δ) # Need to convert to dense matrix to diagonalise
plot(xx, [0; exp(A*t)*u0.(xx[2:end-1]); 0])
plot!(xx, [0; exp(-sqrt(-A)*t)*u0.(xx[2:end-1]); 0])
```
Slow!
```julia
t=10
using BenchmarkTools
@btime exp(A*t)
```

Inverse is fast!
```julia
v0 = u0.(xx[2:end-1])
@time (Δ-0.1im*I) \ v0
```


